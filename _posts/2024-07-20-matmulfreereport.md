---
layout: single
title: "Scalable MatMul-free Language Modeling Report Review by hwaaanss"
---


# 논문 리뷰: Scalable MatMul-free Language Modeling

## 1. 논문 제목 및 저자 정보

- **논문 제목**: Scalable MatMul-free Language Modeling
- **저자**: Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian
- **출판 정보**: 아직 출판되지 않았으며, preprint로 공개됨 (arXiv ID: 2406.02528v5, 2024년 6월)

---

## 2. 연구 배경 및 동기

### 연구 주제와 배경 설명
대형 언어 모델(LLM)은 행렬 곱셈(MatMul) 연산을 중심으로 설계되어 있으며, 이 연산은 파라미터 수와 입력 길이에 따라 계산량이 급격히 증가하여 높은 메모리 사용과 연산 부담을 초래한다. 특히, 수십억 개의 파라미터를 가진 LLM에서는 이러한 연산이 비효율적이며 자원 소모가 크다.

### 연구의 필요성
기존 LLM의 행렬 곱셈 중심 연산은 GPU, TPU 같은 고성능 하드웨어가 필요하며, 자원 소모와 비용이 크다. 이에 따라 연산 자원 효율성을 높이는 새로운 방법이 필요하며, 이러한 효율성 향상을 통해 보다 저전력으로 대형 모델을 실행할 수 있는 방법을 찾는 것이 시급하다.

### 기존 연구와의 차별성
기존 연구들은 주로 MatMul의 효율성을 개선하거나 부분적으로 대체하는 방식에 그쳤다. 본 논문은 최초로 MatMul을 완전히 배제한 LLM 모델을 제안하며, 이를 통해 대규모 모델의 경량화 가능성을 실험적으로 검증했다.

---

## 3. 연구 목적 및 질문

### 연구의 주요 질문 및 가설
- **주요 질문**: MatMul을 사용하지 않고도 대형 언어 모델이 성능을 유지할 수 있는가?
- **가설**: 기존의 LLM에서 필수적인 연산으로 간주된 MatMul을 제거하고, 단순한 덧셈과 Hadamard 곱으로 대체해도 성능 저하 없이 효과적인 언어 모델이 가능할 것이다.

### 연구의 목표
MatMul-free 구조를 통해 메모리 및 연산 자원을 절감하고, LLM의 성능을 유지하거나 개선하는 것을 목표로 한다. 이를 위해 대형 모델의 자원 효율성을 검증하며, MatMul-free 아키텍처가 차세대 하드웨어에 미치는 영향을 평가한다.

---

## 4. 연구 방법 및 실험 설계

### 사용된 방법론 및 모델 구조
1. **Ternary Weights**: Dense 레이어에서 {-1, 0, +1} 값만을 갖는 삼진 가중치를 사용하여, 기존의 MatMul 연산을 단순한 덧셈과 뺄셈으로 대체했다.

   - **Ternary Weights의 정의 및 동작 방식**  
     Ternary Weights는 가중치를 삼진 값 {-1, 0, +1}로 제한하여 곱셈을 단순한 덧셈과 뺄셈으로 대체하는 방식이다. 예를 들어, 기존 Dense 레이어의 행렬 곱셈은 \( y = xW = \sum_{j=1}^{d} x_j W_{ij} \)으로 표현되는데, 여기서 \( W_{ij} \in \{-1, 0, +1\} \)로 가중치를 제한하면 다음과 같이 간단한 덧셈과 뺄셈 연산으로 대체할 수 있다:
     \[
     y = x \odot W = \sum_{j=1}^{d} x_j W_{ij}
     \]
     이를 통해 행렬 곱셈 없이도 가중치를 학습하며, 메모리 사용량과 전력 소모를 크게 줄일 수 있다.

   - **메모리 및 전력 효율성**  
     Ternary Weights는 덧셈과 뺄셈만으로 연산을 수행하므로, 전력 소모가 줄어들고 메모리 절감 효과가 크다. 특히, 덧셈 연산만 포함되므로 자원 소모가 적은 저전력 하드웨어에서도 효율적으로 구동할 수 있다.

   - **안정성 향상을 위한 RMSNorm과 Fused BitLinear 레이어**  
     MatMul-free 구조에서 가중치 불안정을 해결하기 위해 RMSNorm과 Fused BitLinear 레이어가 도입되었다. Fused BitLinear 알고리즘은 RMSNorm과 가중치 정규화 과정을 결합하여 메모리 접근 시간을 줄이고 연산 효율을 높이는 데 기여한다.

2. **MatMul-free Self-Attention**: Attention 연산에서 기존의 MatMul을 Hadamard 곱과 같은 element-wise 연산으로 대체했다.
3. **GRU 기반 Token Mixer**: 토큰 믹싱(token mixing) 단계에서 MatMul을 배제하고, GRU의 element-wise 연산을 통해 정보 통합을 수행했다.

### 실험 데이터 및 환경
- **모델 파라미터 크기**: 370M, 1.3B, 2.7B 파라미터로 모델을 구성하여, 각 크기별로 효율성과 성능을 검증함.
- **훈련 및 테스트 데이터**: SlimPajama 데이터셋을 사용하여 370M 모델에는 15억 개, 1.3B와 2.7B 모델에는 100억 개의 토큰으로 학습.
- **평가 지표**: 모델의 성능은 zero-shot 학습 능력과 메모리 사용량, 연산 속도를 기준으로 평가하였음.

---

## 5. 주요 결과

### 결과 및 발견
- **성능 비교**: MatMul-free 모델은 최대 2.7B 파라미터 모델에서도 Transformer++와 유사한 성능을 보였으며, 대규모 데이터셋을 처리하는 데 있어 성능 손실이 거의 없었다.
- **확장성 검증**: 모델 파라미터가 증가할수록 MatMul-free 모델의 효율성이 더 두드러졌으며, Scaling Law 분석 결과 기존 Transformer++ 모델과 성능 격차가 감소하였다.
- **자원 효율성**: GPU에서 MatMul-free 모델의 메모리 사용량은 기존 모델 대비 최대 61% 절감되었고, 연산 속도는 4.57배 증가하였다.

### 결과의 의미와 해석
대규모 언어 모델에서도 필수적인 것으로 여겨지던 MatMul 연산이 불필요하다는 것을 실험적으로 증명하였다. 이는 자원 효율적이고 저전력으로도 LLM을 구동할 수 있는 가능성을 열어준 연구로 평가된다.

---

## 6. 논문의 기여

- **학문적 기여**: MatMul-free 아키텍처를 통해 대형 언어 모델의 자원 효율성을 극대화할 수 있음을 증명하였으며, 이는 차세대 하드웨어 설계에 대한 중요한 지침이 될 수 있다.
- **실무적 기여**: GPU, FPGA에서 모델을 경량화하여 대형 LLM의 상용화를 위한 전력 소모와 메모리 사용량을 절감하는 방안을 제시하였다.

### 기존 연구 대비 차별성
기존 연구들이 MatMul 효율을 높이기 위해 GPU 최적화에만 집중한 것과 달리, MatMul-free 구조 자체를 도입해 자원 절감의 새로운 가능성을 보여주었다.

---

## 7. 한계점 및 개선 사항

### 연구의 한계점
- **모델 규모 제한**: 본 연구에서는 2.7B 파라미터까지의 모델만 실험하였으나, 초대형 모델 (100B+ 파라미터)에 대한 실험은 진행되지 않았다.
- **연산 최적화 부족**: 현재 FPGA에서 TMATMUL 기능 유닛이 전체 처리 시간의 99%를 차지하는 등 최적화가 미비한 부분이 존재한다.

### 개선 사항 및 제안
향후 연구에서는 초대형 모델에 대한 MatMul-free 구조의 성능을 검증하고, FPGA의 효율을 더욱 높이기 위해 기능 유닛의 병렬 처리와 캐싱 최적화를 통해 처리 속도를 개선할 필요가 있다.

---

## 8. 결론 및 논문의 의의

### 연구의 전체적인 요약 및 결론
본 연구는 대형 언어 모델에서 MatMul 연산을 제거하고, 대체 연산을 통해 자원 절약과 성능을 동시에 달성할 수 있음을 증명하였다. 특히, 대형 모델이 GPU, FPGA 등의 하드웨어에서 더 높은 전력 효율을 보이게 하여, 차세대 LLM의 경량화와 상용화 가능성을 제시하였다.

### 실질적인 의의와 적용 가능성
LLM의 자원 소모를 줄이고도 성능을 유지하는 MatMul-free 아키텍처는 대형 모델의 산업적 활용 가능성을 크게 높이며, 저전력으로도 실행 가능한 새로운 모델 설계 방식에 대한 중요한 기여를 한다.

---

## 9. 개인적인 평가 및 의견

### 논문의 강점
- **혁신성**: MatMul-free 언어 모델은 기존 연구와 차별화되는 획기적인 접근법이며, 실질적인 자원 절감 효과를 보여주었다.
- **실험적 검증**: 다양한 실험을 통해 모델의 확장성과 자원 효율성을 성공적으로 입증하였다.

### 논문의 약점
- **초대형 모델 적용**: 연구가 2.7B 파라미터로 제한되어 초대형 LLM에서의 성능 확인이 필요하다.
- **FPGA 최적화 미흡**: FPGA 최적화가 미비하여 TMATMUL 연산 시간이 지나치게 높은 점은 개선이 필요하다.

### 시사점 및 추가 연구 아이디어
본 연구는 언어 모델 설계에서 MatMul이 꼭 필요하지 않다는 것을 증명하여, 향후 LLM에서 연산 최적화를 위한 새로운 연구 방향성을 제시한다. 추가 연구로는 초대형 모델에 대한 MatMul-free 아키텍처 검증과, 다양한 하드웨어에서의 최적화 방안이 유망하다.
