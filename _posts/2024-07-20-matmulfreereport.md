---
layout: single
title: "논문 리뷰: Scalable MatMul-free Language Modeling by hwaaanss"
mathjax: true
toc: true
toc_sticky: true
toc_label: Scalable MatMul-Free
categories: Paper_Review
tag: [Math, ML]
---


## 1. 논문 제목 및 저자 정보

- **논문 제목**: Scalable MatMul-free Language Modeling
- **저자**: Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, Jason K. Eshraghian
- **출판 정보**: 아직 출판되지 않았으며, preprint로 공개됨 (arXiv ID: 2406.02528v5, 2024년 6월)

---

## 2. 연구 배경 및 동기

### 연구 주제와 배경 설명
대형 언어 모델(LLM)은 행렬 곱셈(MatMul) 연산을 중심으로 설계되어 있으며, 이 연산은 파라미터 수와 입력 길이에 따라 계산량이 급격히 증가하여 높은 메모리 사용과 연산 부담을 초래한다. 특히, 수십억 개의 파라미터를 가진 LLM에서는 이러한 연산이 비효율적이며 자원 소모가 크다.

### 연구의 필요성
기존 LLM의 행렬 곱셈 중심 연산은 GPU, TPU 같은 고성능 하드웨어가 필요하며, 자원 소모와 비용이 크다. 이에 따라 연산 자원 효율성을 높이는 새로운 방법이 필요하며, 이러한 효율성 향상을 통해 보다 저전력으로 대형 모델을 실행할 수 있는 방법을 찾는 것이 시급하다.

### 기존 연구와의 차별성
기존 연구들은 주로 MatMul의 효율성을 개선하거나 부분적으로 대체하는 방식에 그쳤다. 본 논문은 최초로 MatMul을 완전히 배제한 LLM 모델을 제안하며, 이를 통해 대규모 모델의 경량화 가능성을 실험적으로 검증했다.

---

## 3. 연구 목적 및 질문

### 연구의 주요 질문 및 가설
- **주요 질문**: MatMul을 사용하지 않고도 대형 언어 모델이 성능을 유지할 수 있는가?
- **가설**: 기존의 LLM에서 필수적인 연산으로 간주된 MatMul을 제거하고, 단순한 덧셈과 Hadamard 곱으로 대체해도 성능 저하 없이 효과적인 언어 모델이 가능할 것이다.

### 연구의 목표
MatMul-free 구조를 통해 메모리 및 연산 자원을 절감하고, LLM의 성능을 유지하거나 개선하는 것을 목표로 한다. 이를 위해 대형 모델의 자원 효율성을 검증하며, MatMul-free 아키텍처가 차세대 하드웨어에 미치는 영향을 평가한다.

---

## 4. 연구 방법 및 실험 설계

### 사용된 방법론 및 모델 구조
\begin{enumerate}
\item **Ternary Weights**: Dense 레이어에서 {-1, 0, +1} 값만을 갖는 삼진 가중치를 사용하여, 기존의 MatMul 연산을 단순한 덧셈과 뺄셈으로 대체했다.
  
  - **Ternary Weights의 정의 및 동작 방식**  
    Ternary Weights는 가중치를 삼진 값 {-1, 0, +1}로 제한하여 곱셈을 단순한 덧셈과 뺄셈으로 대체하는 방식이다. 예를 들어, 기존 Dense 레이어의 행렬 곱셈은 $y = xW = \sum_{j=1}^{d} x_j W_{ij}$ 으로 표현되는데, 여기서 $W_{ij} \in \{-1, 0, +1\}$ 로 가중치를 제한하면 다음과 같이 간단한 덧셈과 뺄셈 연산으로 대체할 수 있다:

       $$ y = x \odot W = \sum_{j=1}^{d} x_j W_{ij} $$
     
       이를 통해 행렬 곱셈 없이도 가중치를 학습하며, 메모리 사용량과 전력 소모를 크게 줄일 수 있다.
  
  - **메모리 및 전력 효율성**  
    Ternary Weights는 덧셈과 뺄셈만으로 연산을 수행하므로, 전력 소모가 줄어들고 메모리 절감 효과가 크다. 특히, 덧셈 연산만 포함되므로 자원 소모가 적은 저전력 하드웨어에서도 효율적으로 구동할 수 있다.
  
  - **안정성 향상을 위한 RMSNorm과 Fused BitLinear 레이어**  
    MatMul-free 구조에서 가중치 불안정을 해결하기 위해 RMSNorm과 Fused BitLinear 레이어가 도입되었다. Fused BitLinear 알고리즘은 RMSNorm과 가중치 정규화 과정을 결합하여 메모리 접근 시간을 줄이고 연산 효율을 높이는 데 기여한다. Fused BitLinear 알고리즘은 RMSNorm과 가중치 정규화 과정을 결합하여 메모리 접근 시간을 줄이고 연산 효율을 높이는 데 기여한다. 이 알고리즘은 입력 `X`의 평균과 분산을 계산하여 RMS 정규화를 수행하며, 정규화된 활성화 값 `Ỹ`와 가중치 `W`는 양자화 과정을 통해 메모리 사용량을 최적화하고 연산 비용을 줄이는 데 기여한다. 양자화된 활성화와 가중치는 단순한 덧셈 및 뺄셈 연산으로 구성되어 매트릭스 곱셈을 대체하고, 하드웨어 내에서 효율적인 처리를 가능하게 한다.
  
    - **Forward Pass**
      1. $\mu, \sigma^2 \leftarrow \text{mean}(X), \text{variance}(X)$
      2. $r \leftarrow \frac{1}{\sqrt{\sigma^2 + \epsilon}}$
      3. $Ỹ \leftarrow \mathrm{activation\_quant}(r(X - \mu))$
    
    - **Activation Quantization**
      - $s \leftarrow \frac{127}{\text{max}(\left\vert X \right\vert)}$
      - $X̃ \leftarrow \text{round}(sX)$, clamped to range $[-128, 127]$
  
    - **Weight Quantization**
      - $s \leftarrow \frac{1}{\text{mean}(\left\vert W \right\vert)}$
      - $W̃ \leftarrow \text{round}(sW)$, clamped to range $[-1, 1]$
   
    - **Result Computation**
      - $O \leftarrow Ỹ ⊛ W̃ + b$
  
  - 여기서 `⊛` 연산은 MatMul-free 구조에서 단순한 덧셈과 뺄셈으로 수행되며, 하드웨어 내에서 효율적인 처리를 가능하게 한다. 
  
\item **MatMul-free Self-Attention**: Attention 연산에서 기존의 MatMul을 Hadamard 곱과 같은 element-wise 연산으로 대체했다.
\item **GRU 기반 Token Mixer**: 토큰 믹싱(token mixing) 단계에서 MatMul을 배제하고, GRU의 element-wise 연산을 통해 정보 통합을 수행했다.

  - **GRU 기반 Token Mixer**
    GRU(Gated Recurrent Unit)는 순환 신경망(RNN)의 변형으로, LSTM(Long Short-Term Memory)보다 단순하면서 유사한 성능을 제공하는 구조이다. 본 논문에서는 GRU를 기반으로 곱셈 연산을 배제하고 요소별 연산과 누적 덧셈으로 구성된 **MatMul-Free GRU(MLGRU)**를 설계하였다.

  - **표준 GRU 구조**
    기존 GRU는 입력 $x_t$와 이전 은닉 상태 $h_{t-1}$을 사용하여 여러 게이트를 계산한다. 표준 GRU의 수식은 다음과 같다:

    - **Reset Gate**  
      $$ r_t = \sigma(x_t W_{xr} + h_{t-1} W_{hr} + b_r) $$ \\
      이전 은닉 상태 $h_{t-1}$의 정보를 얼마나 재설정할지를 결정하여 현재 입력 $x_t$ 에 기반한 새로운 정보를 반영하는 데 사용된다. 이 게이트는 과거 정보를 얼마나 많이 무시할지를 제어한다. 

    - **Forget Gate**  
      $$ f_t = \sigma(x_t W_{xf} + h_{t-1} W_{hf} + b_f) $$ \\
      LSTM의 forget gate와 유사한 역할을 한다. 이전 은닉 상태 $h_{t-1}$의 정보를 얼마나 유지할지를 제어하여 정보의 소멸 여부를 결정한다.

    - **Candidate Hidden State**  
      $$ c_t = \tanh(x_t W_{xc} + (r_t \odot h_{t-1}) W_{cc} + b_c) $$  \\
      후보 은닉 상태 $c_t$는 현재 입력 $x_t$와 리셋 게이트 $r_t$에 따라 계산되는 잠재적인 새로운 상태이다. 이 상태는 리셋 게이트가 활성화된 이전 은닉 상태 $h_{t-1}$와 입력 $x_t$의 조합을 통해 생성된다. LSTM의 new cell state와 유사한데, LSTM에서는 새로운 후보 상태가 입력 게이트와 결합하여 최종 cell state에 반영될지 결정되고, GRU의 경우 리셋 게이트가 얼마나 이전 은닉 상태가 새 후보 상태에 반영될지를 결정한다.

    - **Final Hidden State**  
      $$ h_t = f_t \odot h_{t-1} + (1 - f_t) \odot c_t $$ \\
      최종 은닉 상태 $h_t$는 망각 게이트 $f_t$의 값과 후보 은닉 상태 $c_t$를 결합하여 이전 은닉 상태 $h_{t-1}$과 새롭게 생성된 상태를 혼합한 결과이다. $f_t$는 이전 은닉 상태 $h_{t-1}$을 얼마나 유지할지를 결정하며, 나머지 부분 $(1 - f_t)$는 새로 계산된 후보 상태 $c_t$가 얼마나 반영될지를 결정한다. 이 gate는 LSTM과 약간의 차이점이 있는데, GRU는 LSTM과 달리 별도의 cell state 없이 최종 은닉 상태 $h_t$가 직접 정보 저장과 출력 역할을 수행하게 된다.

  - **MatMul-Free GRU (MLGRU) 구조**
      MatMul-Free GRU는 곱셈 연산을 제거하고, 단순 덧셈 및 뺄셈 연산으로 대체하여 효율성을 높인 구조이다. 이를 위해 모든 가중치 \( W \)는 삼진 가중치로 제한되며, 이 구조는 다음과 같은 연산으로 이루어진다:

    - **Forget Gate**  
      $$ f_t = \sigma(x_t \odot W_f + b_f) $$  \\
      여기서 $W_f$는 삼진 가중치로 구성되어 있다. **망각 게이트**는 입력 $x_t$와 삼진 가중치 $W_f$의 연산을 통해 이전 은닉 상태 $h_{t-1}$이 얼마나 유지될지를 결정한다. 이 과정은 입력의 특징을 기반으로 과거 정보를 적절히 조절하여 새로운 상태로의 정보 전달을 제어한다.

    - **Candidate Hidden State**  
      $$ c_t = \tau(x_t \odot W_c + b_c) $$ \\
      여기서 $\tau$는 SiLU(Sigmoid Linear Unit) 활성화 함수이다. **후보 은닉 상태**는 리셋 게이트의 작용 없이 현재 입력 $x_t$의 정보를 기반으로 새로운 잠재 상태를 계산한다. 이 단계는 모델이 새롭게 반영할 정보를 생성하며, 복잡한 연산 없이 단순한 요소별 덧셈과 활성화 함수로 처리된다.

    - **Final Hidden State**  
      $$ h_t = f_t \odot h_{t-1} + (1 - f_t) \odot c_t $$  \\
      망각 게이트 $f_t$와 후보 은닉 상태 $c_t$를 기반으로 최종 은닉 상태 $h_t$를 결정한다. 이 과정에서 이전 은닉 상태 $h_{t-1}$의 정보와 새롭게 계산된 후보 은닉 상태 $c_t$가 적절한 비율로 혼합된다. **최종 은닉 상태**는 과거와 현재의 정보가 조화롭게 반영된 상태로, 시퀀스 데이터의 장기적인 패턴을 효과적으로 학습할 수 있게 한다.

    - **Output Gate**  
      $$ g_t = \sigma(x_t \odot W_g + b_g) $$
      $$ o'_t = g_t \odot h_t $$
      $$ o_t = o'_t \odot W_o + b_o $$ \\
      여기서 $W_o$ 또한 삼진 가중치로 이루어져 있으며, $o_t$는 최종 출력이다. **출력 게이트**는 최종 은닉 상태 $h_t$에 대한 가중치를 조절하여 출력 $o_t$를 생성한다. 이를 통해 모델은 선택적으로 은닉 상태의 정보를 출력으로 반영할 수 있으며, 출력 단계에서도 곱셈 없이 연산이 이루어져 하드웨어 자원을 효율적으로 사용할 수 있다.
\end{enumerate}

#### 고찰 및 추가 설명
MatMul-Free GRU는 전통적인 GRU와 비교해 단순화된 구조로, 하드웨어에서 효율적으로 실행될 수 있도록 설계되었다. 곱셈을 제거함으로써 메모리 대역폭과 전력 소모를 줄이는 데 큰 장점이 있으며, 삼진 가중치를 사용해 학습의 복잡성을 줄이면서도 성능 저하를 최소화할 수 있다. 이러한 설계는 대규모 모델을 저전력 환경에서 실행할 때 매우 유리하며, FPGA나 ASIC 같은 특수 하드웨어 가속기에서 높은 효율성을 발휘할 수 있다.


### 실험 데이터 및 환경
- **모델 파라미터 크기**: 370M, 1.3B, 2.7B 파라미터로 모델을 구성하여, 각 크기별로 효율성과 성능을 검증함.
- **훈련 및 테스트 데이터**: SlimPajama 데이터셋을 사용하여 370M 모델에는 15억 개, 1.3B와 2.7B 모델에는 100억 개의 토큰으로 학습.
- **평가 지표**: 모델의 성능은 zero-shot 학습 능력과 메모리 사용량, 연산 속도를 기준으로 평가하였음.

---

## 5. 주요 결과

### 결과 및 발견
- **성능 비교**: MatMul-free 모델은 최대 2.7B 파라미터 모델에서도 Transformer++와 유사한 성능을 보였으며, 대규모 데이터셋을 처리하는 데 있어 성능 손실이 거의 없었다.
- **확장성 검증**: 모델 파라미터가 증가할수록 MatMul-free 모델의 효율성이 더 두드러졌으며, Scaling Law 분석 결과 기존 Transformer++ 모델과 성능 격차가 감소하였다.
- **자원 효율성**: GPU에서 MatMul-free 모델의 메모리 사용량은 기존 모델 대비 최대 61% 절감되었고, 연산 속도는 4.57배 증가하였다.

### 결과의 의미와 해석
대규모 언어 모델에서도 필수적인 것으로 여겨지던 MatMul 연산이 불필요하다는 것을 실험적으로 증명하였다. 이는 자원 효율적이고 저전력으로도 LLM을 구동할 수 있는 가능성을 열어준 연구로 평가된다.

---

## 6. 논문의 기여

- **학문적 기여**: MatMul-free 아키텍처를 통해 대형 언어 모델의 자원 효율성을 극대화할 수 있음을 증명하였으며, 이는 차세대 하드웨어 설계에 대한 중요한 지침이 될 수 있다.
- **실무적 기여**: GPU, FPGA에서 모델을 경량화하여 대형 LLM의 상용화를 위한 전력 소모와 메모리 사용량을 절감하는 방안을 제시하였다.

### 기존 연구 대비 차별성
기존 연구들이 MatMul 효율을 높이기 위해 GPU 최적화에만 집중한 것과 달리, MatMul-free 구조 자체를 도입해 자원 절감의 새로운 가능성을 보여주었다.

---

## 7. 한계점 및 개선 사항

### 연구의 한계점
- **모델 규모 제한**: 본 연구에서는 2.7B 파라미터까지의 모델만 실험하였으나, 초대형 모델 (100B+ 파라미터)에 대한 실험은 진행되지 않았다.
- **연산 최적화 부족**: 현재 FPGA에서 TMATMUL 기능 유닛이 전체 처리 시간의 99%를 차지하는 등 최적화가 미비한 부분이 존재한다.

### 개선 사항 및 제안
향후 연구에서는 초대형 모델에 대한 MatMul-free 구조의 성능을 검증하고, FPGA의 효율을 더욱 높이기 위해 기능 유닛의 병렬 처리와 캐싱 최적화를 통해 처리 속도를 개선할 필요가 있다.

---

## 8. 결론 및 논문의 의의

### 연구의 전체적인 요약 및 결론
본 연구는 대형 언어 모델에서 MatMul 연산을 제거하고, 대체 연산을 통해 자원 절약과 성능을 동시에 달성할 수 있음을 증명하였다. 특히, 대형 모델이 GPU, FPGA 등의 하드웨어에서 더 높은 전력 효율을 보이게 하여, 차세대 LLM의 경량화와 상용화 가능성을 제시하였다.

### 실질적인 의의와 적용 가능성
LLM의 자원 소모를 줄이고도 성능을 유지하는 MatMul-free 아키텍처는 대형 모델의 산업적 활용 가능성을 크게 높이며, 저전력으로도 실행 가능한 새로운 모델 설계 방식에 대한 중요한 기여를 한다.

---

## 9. 개인적인 평가 및 의견

### 논문의 강점
- **혁신성**: MatMul-free 언어 모델은 기존 연구와 차별화되는 획기적인 접근법이며, 실질적인 자원 절감 효과를 보여주었다.
- **실험적 검증**: 다양한 실험을 통해 모델의 확장성과 자원 효율성을 성공적으로 입증하였다.

### 논문의 약점
- **초대형 모델 적용**: 연구가 2.7B 파라미터로 제한되어 초대형 LLM에서의 성능 확인이 필요하다.
- **FPGA 최적화 미흡**: FPGA 최적화가 미비하여 TMATMUL 연산 시간이 지나치게 높은 점은 개선이 필요하다.

### 시사점 및 추가 연구 아이디어
본 연구는 언어 모델 설계에서 MatMul이 꼭 필요하지 않다는 것을 증명하여, 향후 LLM에서 연산 최적화를 위한 새로운 연구 방향성을 제시한다. 추가 연구로는 초대형 모델에 대한 MatMul-free 아키텍처 검증과, 다양한 하드웨어에서의 최적화 방안이 유망하다.
